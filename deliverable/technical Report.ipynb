{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHAT CHARACTERISTICS OF A POST ON REDDIT ARE MOST PREDICTIVE OF THE OVERALL INTERACTION ON A THREAD (AS MEASURED BY NUMBER OF COMMENTS)?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## SUMMARY\n",
    "This report foccuses on posts on reddit, why they attracts lots of interaction(viral). Hence by identifying the contextual and syntactic structure of post is predictive of the overall interactions. Predicting what thread title will go viral and classifying how high or low with respect to the median of the comments within when this text data was scraped. This report concludes that there several factors that are predictive of the overall interaction on a thread and some of those known factors are employed in this analysis.\n",
    "\n",
    "\n",
    "## INTRODUCTION\n",
    "Natural Language Processing has become a usefull tool to extract insight from a text data so as to draw many conclusion from this text data through analysis.\n",
    "The quality of a post thread title, time of posting, subreddit associated to a particular post title can often make or break the popularity of the submission. This post interactions classification model helps shed light on the types of wording, the post structure, the subreddit associated to a particular thread title and time the post lasted that results in popular Reddit comments. So using Natural Language Processing to extract insights from unstructured text of a post and using the words in those text as features for my analysis.\n",
    "For this project i focused on semantic analysis of each reddit post. First i created a classification model that is able to determine high or low the outcome of a post title based on the subreddit associated to that thread title, time post lasted or thread title by learning the contextual and syntactic structure of the post.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## METHADOLOGY\n",
    "\n",
    "### DATA\n",
    "The dataset was scraped from the [reddit.com](https://www.reddit.com/) which contains all reddit submissions (both posts and comments) from February 19, 2018 to February 22, 2018. Web-scrapping was used to extract a total of 1154 text post title of the thread, The subreddit that the thread corresponds to, The length of time it has been up on Reddit, The number of comments on the thread and passed it to a BeautifulSoup object to parse out the 4 fields - title, time, subreddit, and number of comments. Then i Created a dataframe from the results with those 4 columns. made sure i save to a CSV everytime i scraped. So the entire 1154 titles to focus my work on in order to make a proper generalization.\n",
    "\n",
    "\n",
    "### CATEGORIZATION\n",
    "In order to predict the level of the overall interaction on a thread (as measured by number of comments) base on the post's characteristics we use a Random Forest Classifier for the Classification problems, made use of \n",
    "Pipeline and FeatureUnion for combining estimators which i think is useful beacuse there is a fixed sequence of processing my data, Also employed Cross-validatio to evaluate my estimator performance, To avoid overfitting it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set.\n",
    "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset, Also a Kneighbor Classifier at one instance was used just so as to see how a non decision tree based model performs.\n",
    "\n",
    "\n",
    "# RESULTS\n",
    "After training our model on the training data then test our categorization model on the test data. The model acheived a training accuracy of 85% and a test accuracy of 67.6% and good at predicting \"low\" than \"high\".\n",
    "Some subreddits with high featur importance are r/AskReddit, r/Atlanta, and r/aww. This is expected because these subreddits have tokens that unique to their posts. r/AskReddit is mostly a place to ask and answer thought-provoking questions and often contains the token ”?” at the end of a post. some like r/news, contains token ”LIVE” at the front of the post and it contains posts with all kinds of news. r/RussiaLago, Which is talking about Russia that actually makes sense considering what's going on in the political environement in the country.\n",
    "In the case of using threads title to to know if a thread would go high or low viral, To understand why the threads with the most interactions averagaes between 19.5 to 7.3 comments per minute based on counts but some already got 16 to 108 comments per minute seem to show tendency of attracting more comments with time hence learning the contextual and syntactic structure was the obvous thing to do.\n",
    "So i created four new variables based on their availability in the thread title included them in my train and testing which in return increased the model accuracy to 70% but was still good at predicting \"low\" than \"high\"\n",
    "Now i combined the new variables, subreddit and thread title. Considering the political climate of the country now i felt it's best not to ignore the word Russia, Donald, Gun, Fake, Indictment normaly get general attention my model accuracy dropped to 68% accuracy and my model was still better at predicting \"low\" to \"high\" with words like \"does\", \"boy\", \"crazy\" having most feature importance.\n",
    "Looking at the outcome of the KNeighbor Classifier which i have chossen for my process i can conclude that non-tree based methods are not best at classifying High or Low of a post because it's accuracy went further down close to the baseline prediction.\n",
    "Using thread title this time gave an accuracy of 69.3% model was better at predicting \"low\" to \"high\" with \"basement\", \"biology\", \"case\", \"checkmate\", \"does\" having most feature importance. So split my train/test data set into 70/30 this time resulted in a little increase in accuracy 69.7% and as such model became better at predicting \"high\" to \"low\" with features like \"called\", \"boy\", \"brave\" having most importance which seem interesting.\n",
    "\n",
    "\n",
    "\n",
    "## CONCLUTION AND SHORTCOMINGS\n",
    "While the predictors in this analysis contributed to the classification of HIGH or LOW of a post thread title on reddit, I believe more data points should have been considered and i also believe that time a post was put out should also be a factor. The varibles used do not totaly reflect all the possible factors that could contribute to a post interaction. Also threads title, subreddits, and time of post collected over a long period would also help to avoid some that are too small to glean useful insights from.\n",
    "Future work should consider the incorporation of additional features such as using n-grams as inputs,\n",
    "as well as using more sophisticated state-of-the-art language models help improve performance. Finally, hyperparameter tuning can also be optimized using Bayesian\n",
    "methods, which is significantly better than the grid search method we used.\n",
    "My model exhibits 0.72 ~ 72% Specificity, 0.61 ~ 61% Sensitivity, 0.48 ~ 48% Precision and 0.33 ~ 33% Misclassification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
